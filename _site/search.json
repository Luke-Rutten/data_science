[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Luke Rutten",
    "section": "",
    "text": "I’m a 4th year PhD student with scientific expertise in applied statistics as well as human learning, behavior, and motivation. I have a passion for using data to uncover the realities hidden right in front of our eyes and striving - always - to learn and grow as a professional.\nCurrently seeking Summer 2025 Internships.\nEmail me at lrutten20@gmail.com or message me on LinkedIn!\n\nExperience\n\nIntuit | Data Science Intern\n\nConducted large scale simulation studies to validate the use of Bayesian AB testing\nLeveraged PySpark with parallelized cloud computing to cut simulation run time by 94%\nDelivered clear recommendations for default Bayesian priors that would increase decision accuracy by 5-15% and precision of effect size estimates by 2-5x across anticipated business conditions\n\n\n\nUT - Austin | Graduate Research Assistant\n\nDesigned and conducted 8 original studies and assisted with data analysis on 5 additional studies\nSynthesized key takeaways to present at 2 national level conferences, with 4 papers accepted for publication at high impact journals\n\n\n\nUT - Austin | Instructor of Record\n\nDesigned and delivered 30 lectures for a semester-long course, synthesizing complex statistical and theoretical concepts on the working of the human mind\nEarned an average instructor rating, across 84 undergraduate students, of 4.9 / 5\n\n\n\n\nEducation\n\nPhD in Educational Psychology | The University of Texas at Austin | 2021 - 2026\nMA in Quantitative Methods | The University of Texas at Austin | 2021 - 2024\nBS in Psychology & Neurobiology | The University of Wisconsin - Madison | 2017 - 2021"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "",
    "text": "Welcome to the first post of my blog on Data Science!\nYes, yes, I know the title says “practice,” not “data” or “science,” but trust me – it connects. At the time of writing, I’m currently in the process of learning how to be a good (read: hirable) data scientist and it’s likely that I’m going to need to learn a thing or two to get there. As I was having a conversation with someone who knows a lot about how best to go about doing that, I was given two unexpected pieces of advice:\n1) Read “Peak: Secrets From the New Science of Expertise”\n2) Start a blog\nSo. Here I am. Blogging about Peak.\nPeak is a book about developing expertise but, much more accurately than that, it’s a book about practice. Specifically, it’s a book about what the authors call “deliberate practice,” how to do it, and why it’s the most effective path to becoming an expert. This blog post is both a summary of my key takeaways from that book and an extension to how I see it applying to my journey into data science. So, without any further ado, here are some of the highlights from Peak.\nPractice does not make perfect.\nPractice makes permanent.\nPractice takes processes that require conscious attention and creates unconscious copies so that next time, we don’t need to think. We can just do. Practice makes our actions faster. And easier. And automatic. Therefore, to practice effectively, we need to (1) identify the exact behaviors we want automated and (2) practice them until they are.\nNow, the interesting thing about practice is that not all forms are created equal. At its core, practice depends on neurobiological mechanisms designed to help our brains grow faster and more energy efficient. In much the same way that we can tailor our workout plans to build better muscles, we can also tailor our practice to make use of these mechanisms and, in doing so, push ourselves to improve even faster and even further than we would ever have otherwise been able to go.\nThis is the process known as “deliberate practice” and this is the concept that this blog post is dedicated to."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Data Science",
    "section": "",
    "text": "Calculating Better Buzzfeed Quizzes\n\n\n\n\n\n\nStatistics\n\n\n10 Minute Read\n\n\n\nThe secret ingredient is multivariate statistics\n\n\n\n\n\nNov 5, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\n\n\n\n\n\n\nThe shadow CLT\n\n\n\n\n\n\nStatistics\n\n\n3 Minute Read\n\n\n\nA statistical secret hiding within every population\n\n\n\n\n\nOct 22, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\n\n\n\n\n\n\ntesting\n\n\n\n\n\n\n\n\n\n\n\nOct 22, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Models Make Assumptions About Residuals, Not Data. Why?\n\n\n\n\n\n\nStatistics\n\n\n10 Minute Read\n\n\n\nExploring the reasons our models make assumptions not about our data at all, but about the residuals they produce\n\n\n\n\n\nOct 9, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Nested For-Loops in Python and R\n\n\n\n\n\n\nPython\n\n\nR\n\n\nProgramming\n\n\n3 Minute Read\n\n\n\nMake iteration easy to write, easy to read, and easy to parallelize with these simple functions\n\n\n\n\n\nOct 2, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\n\n\n\n\n\n\nDeliberate Practice - Getting Better, Faster\n\n\n\n\n\n\nIntroduction\n\n\nStrategizing\n\n\n\nSummarizing a good book and extracting its key ideas.\n\n\n\n\n\nMay 20, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html#find-a-good-teacher",
    "href": "posts/welcome/index.html#find-a-good-teacher",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "1) Find a good teacher",
    "text": "1) Find a good teacher\nA good teacher is invaluable. They will be able to help you identify both the most useful things to work on as well as the ways in which you can improve what you’re doing. Oftentimes, a good teacher is someone who knows what you’re trying to accomplish well; someone who’s accomplished it themselves or someone who’s trained to help others reach that accomplishment before. Whatever it is you’re trying to do, it will likely help to have someone who knows more than you directing your efforts as you go."
  },
  {
    "objectID": "posts/welcome/index.html#identify-something-specific-to-work-on",
    "href": "posts/welcome/index.html#identify-something-specific-to-work-on",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "2) Identify something specific to work on",
    "text": "2) Identify something specific to work on\nIdeally, work with your teacher to identify this thing.\nIn order to build a mental representation, we need to make the neurons involved activate together over and over. The best way to make that happen is to do something that depends on the representation… over and over. The more times we get our brain to reach for the representation, the stronger the signal that this group of neurons are connected will be and, therefore, the more quickly those connections will be solidified.\nIf we go into practice without a specific goal in mind, it’s likely that we’ll spend our time doing numerous different things, bouncing around from one task to another. However, approaching practice like this doesn’t give our brain the chance to reach for a specific mental representation over and over and, in line with that, doesn’t give our brain the signal it needs to really solidify the representation.\nFurthermore, without a specific goal, it can be difficult to identify ways in which to adjust what we’re doing to improve towards that goal. Without these adjustments, continued practice will just make it easier and faster to do the things we’re already doing. If what we’re already doing is correct, this is wonderful and no adjustments are needed. If, however, it is instead the case that we’re doing something wrong, then making those neuronal connections stronger actually turns out to be detrimental to us down the line. In all likelihood, if you’re still learning how to do something, your current approach is not optimal. It probably needs to be changed. Identifying specific goals to work towards and making attempts over and over will not only begin to solidify mental representations related to this goal but will also allow you to identify and highlight (and practice) ways to change what you’re doing before it’s been made too permanent."
  },
  {
    "objectID": "posts/welcome/index.html#focus-100-while-you-practice",
    "href": "posts/welcome/index.html#focus-100-while-you-practice",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "3) Focus 100% while you practice",
    "text": "3) Focus 100% while you practice\nThis one is straightforward. The more you can focus on what you’re practicing, the stronger the signal will be that this is important. The stronger that signal is, the quicker the mental representations will be built.\nFurther, letting yourself lapse and get sloppy with attempts will mean the wrong sets of neurons are activating together. In essence, if you’re giving anything less than your best, you’re building mental representations of subpar behaviors. Unfortunately, that’s going to make it that much harder in the future when you’re trying to learn more advanced skills that depend on these lower-level representations. Don’t let yourself be sloppy. Focus, 100%.\nNow, with that said, it’s difficult to push yourself at 100% focus for long periods of time. Instead, limiting practice sessions to about an hour and having multiple sessions separated by generous breaks makes it much more feasible."
  },
  {
    "objectID": "posts/welcome/index.html#get-feedback-and-make-it-as-objective-as-possible",
    "href": "posts/welcome/index.html#get-feedback-and-make-it-as-objective-as-possible",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "4) Get feedback and make it as objective as possible",
    "text": "4) Get feedback and make it as objective as possible\nQuantify your performance. Then use that information to adjust as you go. Are you making progress? At the rate you expect? Is your practice working or have you reached a plateau? Knowing what’s working and what isn’t is essential for creating and updating plans as you go. Unfortunately, humans are notoriously bad at making objective assessments of things. Instead of depending on biased intuitions, get concrete performance feedback whenever possible and use that information to help make decisions."
  },
  {
    "objectID": "posts/welcome/index.html#additional-considerations",
    "href": "posts/welcome/index.html#additional-considerations",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "Additional Considerations",
    "text": "Additional Considerations\nAt this point, you’ve received a surface-level summary of deliberate practice, why it works, and how to do it. Though I’ve skipped over many of the details in creating this summary, what follows is a list of what I see as the details that are essential to mention:\n\nSleep is so important to this process. Get a full night’s rest every night, or your brain won’t have the time it needs to use the mechanisms that learning depends on.\nThe social environment exerts a powerful influence on our behaviors. Surround yourself with people who actively encourage and support your growth (not distract you from it). Encourage them, in turn. Creating communities of support helps both you in your own endeavors and the world around you.\nAim to work on skills that are just outside of your comfort zone. They should push you, but—with focus—you should be able to accomplish them. These are the skills for which your mental representations aren’t quite built yet. As you work on them and master them, they will lay the foundations for all the steps that come after and make learning those skills much more efficient.\nIf plateauing, try differently, not harder. Ideally, you’ll already be at 100% effort anyway. When approaching a barrier you can’t seem to overcome, changing your angle may help you see things you weren’t able to before.\nDuring practice, actively searching for mistakes is critical.\n\nThis helps keeps you 100% focused instead of zoning out.\nThis also helps you identify and correct errors before they become too ingrained."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Luke Rutten",
    "section": "Experience",
    "text": "Experience\n\nIntuit - | - Data Science Intern - | - Summer 2024\n\nConducted large scale simulation studies to determine safe default priors for Bayesian AB testing\nLeveraged massive parallel processing using PySpark in Databricks to reduce simulation run time by 94%\nDelivered clear recommendations for default Bayesian priors to increase decision accuracy by 5-15% and increase effect size estimate precision by 2-5x across anticipated business conditions\n\nUT - Austin - | - Graduate Research Assistant - | - August 2021 - Present\n\nDesigned and conducted 8 original studies and assisted with data analysis on 5 additional studies\n\nMethods included: Hierarchical modeling, latent variable modeling, random forest regression, MANOVA\n\nSynthesized key takeaways to present at 2 national level conferences, with 5 papers under review at high impact journals\n\nUT - Austin | Instructor of Record | 2023 - 2024\n\nDesigned and delivered two 75 minute lectures each week over the course of a 15 week semester\nEarned an average instructor rating of 4.9 / 5"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Luke Rutten",
    "section": "Education",
    "text": "Education\n\nPhD in Educational Psychology | The University of Texas at Austin | Aug 2021 - May 2026\nMA in Quantitative Methods | The University of Texas at Austin | Aug 2021 - May 2026\nBS in Psychology & Neurobiology | The University of Wisconsin - Madison | Aug 2017 - May 2021"
  },
  {
    "objectID": "posts/simplying_for_loops/index.html",
    "href": "posts/simplying_for_loops/index.html",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "",
    "text": "The Quick Answer:\nUse specific functions to create a data frame of the Cartesian product of vectors passed to them, which can then be iterated across with just one loop (or parallelized).\nThe function in Python: itertools.product()\nThe function in R: tidyr::expand_grid()\n\n\nThe Full Explanation:\nLet’s say you’re running a simulation study and you’ve been tasked with gathering performance data across a wide range of conditions. Maybe you’re working for a car company that’s trying to perfect its new line of fuel injectors and you need to assess the efficacy of the product across the full range of:\n\nEngine sizes\nEngine temperatures\nManufacturers\nLevels of remaining fuel\nCar mileages\nCar speeds\n\nTo capture performance across the full range of possible conditions here, you’re going to need to run hundreds or maybe even thousands of simulations! That’s way too many to try running one at a time but, lucky for you, you know a thing or two about programming. So you go ahead and define your parameters, then write some for-loops to iterate across conditions.\nLet’s check out a simplified version of what that might look like.\n\nSet parameters\nYou don’t technically need to define your parameters outside the loop, but doing so makes for code that’s both more readable and easier to modify later.\n\nPythonR\n\n\n\nengine_sizes = [2.4, 3.2]\nengine_temps = [180, 200, 220]\nmanufacturers = ['Toyota', 'Mazda', 'Honda']\nfuel_levels = [0.25, 0.75]\nmiles = [50000, 100000, 150000]\nspeeds = [30, 50, 80]\n\n\n\n\nengine_sizes = c(2.4, 3.2)\nengine_temps = c(180, 200, 220)\nmanufacturers = c(\"Toyota\", \"Mazda\", \"Honda\")\nfuel_levels = c(0.25, 0.75)\nmiles = c(50000, 100000, 150000)\nspeeds = c(30, 50, 80)\n\n\n\n\nAnd now that your parameter space is defined, you can go ahead and iterate across it.\n\n\nNested for-loops\n\nPythonR\n\n\n\n# Write nested for-loops to capture every parameter combination\nfor size in engine_sizes:\n  for temp in engine_temps:\n    for manufac in manufacturers:\n      for fuel in fuel_levels:\n        for mile in miles:\n          for mph in speeds:\n        \n            some_function(arg_1 = size, arg_2 = temp, arg_3 = manufac, arg_4 = fuel, arg_5 = mile, arg_6 = mph)\n\n\n\n\n# Write nested for-loops to capture every parameter combination\nfor (size in engine_sizes){\n  for (temp in temps){\n    for (manufac in manufacturers){\n      for (fuel in fuel_levels){\n        for (mile in miles){\n          for (mph in speeds){\n        \n            some_function(arg_1 = size, arg_2 = temp, arg_3 = manufac, arg_4 = fuel, arg_5 = mile, arg_6 = mph)\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nThis code has been simplified for the sake of the example, but we’re already beginning to see an issue. Code that’s been indented 6 times is just hard to read! Now imagine a problem where you have to take into account not just 6 parameters, but 10 or 20 or even 30.\nIt would be a nightmare.\nBut it’s not like we can avoid iteration just because it’s hard to read, so what can be done?\n\n\nThe Solution: Cartesian Products\nBecause a single for-loop is limited to iteration across just one dimension, we need a way to condense all of our parameters (6, in this example) into just one dimension. This could be done as a data frame, a list of lists, or perhaps even as a dictionary of parameters.\nWhichever route we choose to go, it’s not enough just to include our parameters separate from one another. Rather, we need each value to contain a unique combination of parameters and we need every unique combination to be present.\nLuckily for us, most coding languages (and the communities supporting them) have already laid the groundwork to enable us to do this with ease. In Python, we can use itertools.product() and, in R, we can use tidyr::expand_grid(). These functions take the data frame approach, calculating every unique combination of parameters and storing the output in individual rows of a data frame.\n\nPythonR\n\n\n\nengine_sizes = [2.4, 3.2]\nengine_temps = [180, 200, 220]\n\npd.DataFrame(itertools.product(engine_sizes, engine_temps))\n\n     0    1\n0  2.4  180\n1  2.4  200\n2  2.4  220\n3  3.2  180\n4  3.2  200\n5  3.2  220\n\n\n\n\n\nengine_sizes = c(2.4, 3.2)\nengine_temps = c(180, 200, 220)\n\ntidyr::expand_grid(engine_sizes, engine_temps)\n\n# A tibble: 6 × 2\n  engine_sizes engine_temps\n         &lt;dbl&gt;        &lt;dbl&gt;\n1          2.4          180\n2          2.4          200\n3          2.4          220\n4          3.2          180\n5          3.2          200\n6          3.2          220\n\n\n\n\n\nAnd once we have that, all we need to do is iterate across each row of the newly created data frame! Each row contains all the parameter information we’ll need, so we just extract that information within the loop as we iterate through it.\nNow, putting it all together, here’s what code for that might look like:\n\nPythonR\n\n\n\n# load necessary packages\nimport pandas as pd\nfrom itertools import product\n\n# Create a single data frame using the Cartesian product of your parameters\nparams = pd.DataFrame(product(engine_sizes, engine_temps, manufacturers,\n                              fuel_levels, miles, speeds))\n\n# Run just one loop across each row of your data frame\nfor row in np.arange(params.shape[0]):\n  some_function(arg_1 = params.iloc[row, 0],\n                arg_2 = params.iloc[row, 1],\n                arg_3 = params.iloc[row, 2],\n                arg_4 = params.iloc[row, 3],\n                arg_5 = params.iloc[row, 4],\n                arg_6 = params.iloc[row, 5])\n\n\n\n\n# load necessary packages\nlibrary(tidyverse)\n\n# Create a single data frame using the Cartesian product of your parameters\nparams = expand_grid(engine_sizes, engine_temps, manufacturers,\n                     fuel_levels, miles, speeds)\n\n# Run just one loop across each row of your data frame\nfor (row in nrow(params)) {\n  some_function(arg_1 = params[[row, 1]],\n                arg_2 = params[[row, 2]],\n                arg_3 = params[[row, 3]],\n                arg_4 = params[[row, 4]],\n                arg_5 = params[[row, 5]]\n                arg_6 = params[[row, 6]])\n}\n\n\n\n\nAnd it’s really that simple.\nUsing this approach, it doesn’t matter how many parameters you’re working with; the amount of loops you need will only ever be one.\n\n\nThe True Power of this Approach\nAs the experienced reader may have already realized, this approach not only makes it easier to read and write nested for-loops, it also makes it much simpler to parallelize them. With nested loops, you’re limited to parallelizing just one level at a time or worrying about nested threads if you’re brave enough to dare trying multiple. With the Cartesian approach, however, you can reach every combination of parameters with just one parallelized operation, making full use of your computing resources and freeing up your time and attention to think about more important things like how in the world you’re going to visualize all this awesome data you’ve just collected.\nThat, however, is a topic for another post ;)"
  },
  {
    "objectID": "posts/simplying_for_loops/index.html#results",
    "href": "posts/simplying_for_loops/index.html#results",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "Results",
    "text": "Results\n\nPlots\ntesxt\n\n\nTables\ncxd"
  },
  {
    "objectID": "posts/simplying_for_loops/index.html#something-different",
    "href": "posts/simplying_for_loops/index.html#something-different",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "Something different",
    "text": "Something different\nNot a tabset or something\nOne thing to note: with this much power, it’s very easy to allow things to get out of hand. Each new condition increases exponentially the amount of conditions to iterate through.\nBecause this approach allows each condition to run separately from the others, it lends itself directly to parallelization - but that’s a topic for another blog post."
  },
  {
    "objectID": "posts/simplying_for_loops/index.html#parallelization---the-true-power-of-this-approach",
    "href": "posts/simplying_for_loops/index.html#parallelization---the-true-power-of-this-approach",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "Parallelization - The True power of this approach",
    "text": "Parallelization - The True power of this approach\nNot a tabset or something\nOne thing to note: with this much power, it’s very easy to allow things to get out of hand. Each new condition increases exponentially the amount of conditions to iterate through.\nBecause this approach allows each condition to run separately from the others, it lends itself directly to parallelization - but that’s a topic for another blog post."
  },
  {
    "objectID": "posts/normal_residuals/index.html",
    "href": "posts/normal_residuals/index.html",
    "title": "Linear Models Make Assumptions About Residuals, Not Data. Why?",
    "section": "",
    "text": "Perhaps one of the most often misunderstood and misrepresented concepts in statistics is that of what we actually make assumptions about. All too frequently, ___ and well meaning stats-help sites will say that we want normally distributed data.\nThat’s wrong.\nLinear models don’t really care how the data is distributed. What they actually care about is how their residuals—the errors in their estimates—are distributed. But why? Why do we care about our models’ errors and why are those often more informative than the data itself?"
  },
  {
    "objectID": "posts/normal_residuals/index.html#potentially-a-section-in-this-on-the-goal-of-models",
    "href": "posts/normal_residuals/index.html#potentially-a-section-in-this-on-the-goal-of-models",
    "title": "Why Do We Make Assumptions About Residuals, Not Data?",
    "section": "Potentially a section in this on the goal of models",
    "text": "Potentially a section in this on the goal of models\n\naverages\nand also significance of those averages"
  },
  {
    "objectID": "posts/normal_residuals/index.html#regression-math",
    "href": "posts/normal_residuals/index.html#regression-math",
    "title": "Why Do We Make Assumptions About Residuals, Not Data?",
    "section": "Regression Math",
    "text": "Regression Math\nBefore we dive into the conceptual reasonings behind residual-centered assumptions, let’s take a quick look at the math going on under their hoods. For this explanation, we’ll focus on one of the more basic forms: simple linear regression.\nThis model aims to estimate the relationship between two variables, predicting how one will change as a linear function of the other. It’s mathematical formula is:\n\\(y = \\beta_0 + \\beta_1 x_1 + \\epsilon\\) where \\(\\epsilon \\sim N(0, σ^2)\\) $$\nIs this group different than that group? What are you really asking? - Is this group, on average, different? - If you chose one from each, are you more likely to see a difference?\nMany of these methods ask about average differences between groups - a subtle, but important distinction."
  },
  {
    "objectID": "posts/normal_residuals/index.html#footnotes",
    "href": "posts/normal_residuals/index.html#footnotes",
    "title": "Linear Models Make Assumptions About Residuals, Not Data. Why?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe also make assumptions that allow us to believe an accurately estimated average is meaningful (e.g., there truly is a linear relationship between variables when running a linear regression).↩︎\nCalculating an average with outliers present is not a problem for the math itself, however it may be a problem for interpreting the output. It all depends on the question you’re trying to answer. If what you care about is truly the population average, then it’s fine. Often though, we’re focused on modelling “normal” or “business-as-usual” data that outliers don’t fit the pattern for. In these cases, the presence of outliers can lead to correct but misleading averages.↩︎"
  },
  {
    "objectID": "posts/normal_residuals/index.html#the-mathematical-answer",
    "href": "posts/normal_residuals/index.html#the-mathematical-answer",
    "title": "Linear Models Make Assumptions About Residuals, Not Data. Why?",
    "section": "The Mathematical Answer",
    "text": "The Mathematical Answer\nBefore we dive into the conceptual reasonings behind residual-centered assumptions, let’s take a quick look at the math going on under their hoods. For this explanation, we’ll focus on one of the more basic forms: simple linear regression. I’m going to assume you’re already familiar with the notation below, but if you’d like a quick refresher, this site does a good job of explaining the formula (though be sure to note how they get the assumption of normally distributed data wrong!). In any case, here’s the mathematical formula:\n\\[\ny = \\beta_0 + \\beta_1 x_1 + \\epsilon\n\\]\nWhere:\n\\[\n\\epsilon \\sim N(0, \\sigma^2)\n\\]\nFrom this, we can make three immediate observations:\n\nThis formula makes no claim about the distribution of \\(x_1\\). It could be anything.\n\\(y\\) is a linear function of \\(x_1\\), implying that the distribution of \\(y\\) will resemble the distribution of \\(x_1\\), whatever \\(x_1\\) happens to be (with an error term allowed, of course).\nThe error term must follow a normal distribution, centered around 0, with some constant variance.\n\nAlright! So with these observations in mind, we can now make a number of claims. First, it doesn’t matter what the distribution of our data (\\(y, x_1\\)) looks like. Our variables could be normal, they could be uniform, they could even be random peaks scattered across infinity. So long as \\(x_1\\) and \\(y\\) are linearly related to one another, the math will work. Second, it does matter what the distribution of our errors looks like.\nSo, then, to answer the original question of why we make residual-focused assumptions:\nThe math said so.\nConvincing, isn’t it?"
  },
  {
    "objectID": "posts/normal_residuals/index.html#what-are-the-models-goals",
    "href": "posts/normal_residuals/index.html#what-are-the-models-goals",
    "title": "Linear Models Make Assumptions About Residuals, Not Data. Why?",
    "section": "What are the model’s goals?",
    "text": "What are the model’s goals?\nTo understand why we make the assumptions we make when running these models, we need to understand what it is that the models are trying to accomplish. Of course, different models aim to do different things but, generally speaking, their goals are usually to (1) estimate some kind of average (e.g., average difference between groups, average change of one variable across changes in another) and (2) estimate the significance of that average (assuming the null is true, how likely would we be to see an average this extreme or greater due to random noise?).\nIn line with that, the assumptions we make when running these models are the assumptions necessary for us to (1) believe we have accurately estimated the average and (2) believe that we have accurately estimated the significance level of that average 2."
  },
  {
    "objectID": "posts/normal_residuals/index.html#the-conceptual-answer",
    "href": "posts/normal_residuals/index.html#the-conceptual-answer",
    "title": "Linear Models Make Assumptions About Residuals, Not Data. Why?",
    "section": "The Conceptual Answer",
    "text": "The Conceptual Answer\nWell, if you’re anything like me, the mathematical answer probably isn’t the most satisfying conclusion. Understanding how the math was written doesn’t help us understand why the math was written that way. So, if you’re still wondering what the thinking behind these assumptions is, this section is dedicated to you.\nTo understand our focus on residuals, we first need to understand what exactly our models are trying to accomplish. Generally speaking, their goals are to (1) estimate some kind of average (average difference between groups, average change of one variable across changes in another) and (2) estimate the significance of that average (assuming the null is true, how likely would we be to see an average this extreme or greater due to random noise?).\nIn line with that, the assumptions we make when running these models are the assumptions necessary for us to (1) believe we have accurately estimated the average and (2) believe we have accurately estimated the significance of said average1.\n\n1) To accurately estimate the average\nIn order to calculate an average, you need data that’s at least at the interval level (an implied assumption in most of these models), but otherwise… you’re pretty much good to go. Is the data normally distributed? Doesn’t matter. Do you have outliers? Doesn’t matter 2. Do independent groups have the same variance? Still doesn’t matter. Because an average is just the sum of inputs divided by the number of inputs, the data can be in whatever form it wants to be and you’ll still get your average.\nOk, so we don’t need to make any assumptions about the distribution of the data itself to calculate an average, but how do we know the averages our model reports are accurate to the data? In other words, how do we know we our model isn’t biased?\n\nBiased ModelUnbiased Model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPut simply, the easiest way to tell if your model’s estimates are biased is to check whether the mistakes it makes average out to 0. If they don’t, that means your model is making a systematic over- or under-estimation.\nIn the biased model above, the prediction (that solid black line) is pretty consistently below the actual data (those red dots). Its residuals - the errors or the differences between its prediction and the actual data - do not average out to 0. The residuals of the unbiased model do.\nAha! So there’s the first reason we pay attention to residuals and not the data - we don’t need the data to be structured any particular way to calculate an average, but we do need the errors structured a certain way (balancing each other out to 0) to ensure our estimate of that average is unbiased.\n\n\n2) To accurately estimate the significance\nHere’s where the real fun begins.\nAccurately estimated significance requires residuals to follow a specific form.\nFirst, though, let’s recap: what is significance? Significance is a representation of how likely we would be to see a value this extreme or greater, assuming the null hypothesis is true. In other words, sometimes random chance makes it looks like something is going on even when it isn’t; how likely would random chance be to make our data look the way it does (or even more extreme) if nothing was actually going on?\nTo calculate the chance of this happening, we create a probability distribution for the null hypothesis - a distribution that tells us how likely we would be to see each potential outcome due to random fluctuations in sampling. We then check where our observed data lands on that distribution and that tells us how likely (or unlikely) random chance is to have created our data.\nThat’s important.\nOur calculated likelihood depends on the probability distribution we use. Because of this, we could observe the exact same average across different data sets, but make very different claims about that average’s significance depending on how the null distribution turns out. If it’s a distribution with high variance, claims about significance get weaker. If it’s a distribution with tight variance though, claims about significance are much stronger.\n\nHigh VarianceMedium VarianceLow Variance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo each of these null distributions give us a different answer. Now, how do we know which one to use?\nShould we use the wide one? The narrow one? Something entirely different? Which one we choose directly affects the significance result, so we’d better make sure we get it right.\n\nTalk about normal residuals\n\nMention CLT\n\nTalk about homoscedasticity\n\nWith residuals - take a linear relationship, what’s the variance of the data? - What’s the variance of the residuals?\nSo we calculate the null distribution using error terms as a parameter in a normal distribution. But what happens if our errors aren’t normally distributed? Well now we’re comparing apples to oranges.\n\nuniform predictor + normal error goes here -\n\nYour model’s errors don’t have to follow the exact same distribution as the null hypotheses’ but, if they don’t, the statistics used to calculate the null hypothesis distribution which your model is then compared to is wrong.\n\nnormal predictor + uniform error goes here -\n\nIt’s hard to see this with just one example, so let’s run a quick simulation study. We want to calculate this statistic with 80% power and 5% type 1 error rate. Let’s just see what happens.\nNow, here’s something the astute reader may have noticed earlier - in our scatter plot showing biased and unbiased models, our data was NOT normally distributed. It was uniformly distributed, with normal errors!\n\nsim study goes here -\n\nThe model with uniform errors performed worse, because the hypothetical error distribution (null hypothesis) to which it was compared did not match its own actual error distribution.\n\nPanel 1Panel 2"
  },
  {
    "objectID": "posts/normal_residuals/index.html#to-accurately-estimate-the-average",
    "href": "posts/normal_residuals/index.html#to-accurately-estimate-the-average",
    "title": "Linear Models Make Assumptions About Residuals, Not Data. Why?",
    "section": "To accurately estimate the average",
    "text": "To accurately estimate the average\nIn order to calculate an average, you need data that’s at least at the interval level (an unwritten assumption in most of these models), but otherwise… you’re pretty much good to go. Is the data normally distributed? Doesn’t matter. Do you have outliers? Doesn’t matter. Do independent groups have the same variance? It doesn’t matter. Because an average is just the sum of inputs divided by the number of inputs, the data can be in whatever form it wants to be and you’ll still get your average.\nOk, so we don’t need to make any assumptions about the distribution of the data itself to calculate an average, but how do we know the averages calculated by the model are unbiased? Well, for that, we would need to see that errors in the model cancel each other out. In other words, we need to see that the residuals are centered around 0.\nAha! So there’s the first reason we pay attention to residuals and not the data - we don’t need the data to be structured any particular way to calculate an average, but we do need the residuals structured a certain way (centered on 0) to make sure our estimate of that average is unbiased.\n\nTo accurately estimate the significance\nNow here’s where the real fun begins.\nWhat is significance? It’s a representation of how likely we would be to see an average of this extreme or greater due to random sampling, assuming the null hypothesis was true. In order to calculate that probability, we rely on a distribution. That distribution is calculated using properties of the residuals?\nYou can think of the null hypothesis distribution as a form of error itself - we know what the “truth” is (it’s the null hypothesis), and we’re modelling how likely each error is to exist from there.\nWe need errors to be iid for this. Otherwise, which variance should we use? It’s like comparing apples to oranges. There’s no way of modelling it that really captures what we’re after.\n\nuniform predictor + normal error goes here -\n\nYour model’s errors don’t have to follow the exact same distribution as the null hypotheses’ but, if they don’t, the statistics used to calculate the null hypothesis distribution which your model is then compared to is wrong.\n\nnormal predictor + uniform error goes here -\n\nIt’s hard to see this with just one example, so let’s run a quick simulation study. We want to calculate this statistic with 80% power and 5% type 1 error rate. Let’s just see what happens.\n\nsim study goes here -\n\nThe model with uniform errors performed worse, because the hypothetical error distribution (null hypothesis) to which it was compared did not match its own actual error distribution.\n\nPanel 1Panel 2"
  },
  {
    "objectID": "posts/clts_all_the_way_down/index.html",
    "href": "posts/clts_all_the_way_down/index.html",
    "title": "The shadow CLT",
    "section": "",
    "text": "I should start by saying that although the Shadow CLT is a real phenomenon, you’ll probably never find yourself in a situation where it matters. However, I also think it gives some insights into how sampling, populations, and the CLT work - so, if that sounds of interest to you, I encourage you to read on!\n\nThe Central Limit Theorem\n\nAs you collect samples from a population, the Central Limit Theorem states that the distribution of the means of those samples will converge to a normal distribution—regardless of the underlying population distribution—so long as the sample sizes are large enough.\n\nNow that CLT definition is a bit abstract so, to make it more clear, here’s a demonstration. Let’s say we’re trying to make an inference about the average height of students in their freshman year, but there’s a small problem: human height tends to follow a bimodal distribution (one peak for males, one peak for females).\n\n\nCode\n# Load libraries\nimport numpy as np\nimport pandas as pd\nimport plotnine as pn\n\n# Simulate data\nh_males = pd.DataFrame({'height': np.random.normal(loc = 176, scale = 2.7, size = 10000)})\nh_females = pd.DataFrame({'height': np.random.normal(loc = 166, scale = 2.5, size = 10000)})\n\nheights = pd.concat([h_males, h_females], axis = 0)\n\n# Plot data\n(pn.ggplot(heights, pn.aes(x = 'height')) +\n  pn.geom_density(fill = 'grey', alpha = .7) +\n  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +\n  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights') +\n  pn.theme_bw() + \n  pn.theme(axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_text_x = pn.element_text(size = 12),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),\n        title = pn.element_text(size = 16))\n)\n\n\n\n\n\n\n\n\n\nThe non-normal nature of the data is going to affect our variance estimates which will, in turn, affect our significance estimates. Not to worry though! The Central Limit Theorem states that, even from this non-normal distribution, if we were to sample from it over and over and then plot the means of those samples, the outcome would end up normal so long as the size of our samples is large enough.\nLet’s try it! We’ll take samples of varying sizes (n), calculate the sample averages, and then plot them out and see what happens:\n\nn = 1n = 30n = 300\n\n\n\n\nCode\n# Create dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = heights.sample(n = 1)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = heights.sample(n = 30)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = heights.sample(n = 300)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\nIt works!\nEven when drawing samples from a population that is very much not normally distributed, the means of our samples ended up looking normal. The CLT says this will hold true pretty much no matter what the population distribution looks like; if it has a mean and a variance, the sample means will end up normal as long as your sample size is large enough. That’s a good thing too, because many of our statistical practices actually depend on the assumption that this holds up.\nAnd, in most cases, it does. Except for, well…\n\n\nThe Shadow CLT\nIt turns out there’s a unique spot within every population where the CLT begins to fall apart and why this happens is because of a secret, inverted CLT which I’m unofficially terming “The Shadow CLT” (mostly because I find that name funny hahaha). Rather than dive into a conceptual explanation, though, let me just show the Shadow CLT in action:\nWe start with a full population of 1,000 data points. It doesn’t need to be 1,000; it could be 42 or 1,000,000 or really however many you feel like. I’m going with 1,000 because it’s easy to graph. Now, in this example, our thousand data points follow a non-normal distribution like before. Maybe we’re trying to make an inference about the average height of freshman attending a specific school this time and our population data looks something like this:\n\n\nCode\n# Simulate data reproducibly\nnp.random.seed(11)\n\nh_males = pd.DataFrame({'x1': np.random.normal(loc = 177, scale = 2.7, size = 500)})\nh_females = pd.DataFrame({'x1': np.random.normal(loc = 164, scale = 2, size = 500)})\n\nsims = pd.concat([h_males, h_females], axis = 0)\n\n# Plot it\n(pn.ggplot(sims, pn.aes(x = 'x1')) +\n  pn.geom_density(fill = 'grey', alpha = .7) +\n  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +\n  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights (1,000 Students)') +\n  pn.theme_bw() + \n  pn.theme(axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_text_x = pn.element_text(size = 12),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),\n        title = pn.element_text(size = 16))\n)\n\n\n\n\n\n\n\n\n\nRemember, the central limit theorem says that as long as our sample size is large enough, the distribution of our sample means should end up looking normal here.\nThe Shadow CLT doesn’t quite agree, but before we get to that let’s first establish that the normal CLT works within this fictional dataset of 1,000 freshmen. Again, we’ll take a bunch of samples of various sizes (n) and plot them out so we can see how they’re distributed.\n\nn = 1n = 2n = 30\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 1)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 2)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 30)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\nGreat! Everything is looking good so far. Our n = 1 and n = 2 distributions weren’t exactly normal but that’s because the sample sizes weren’t large enough for the CLT to take effect. Once we got up to samples of 30, our distribution started to take on a nice, normal shape.\nAnd as we add even more data to our samples, things only get better.\n\nn = 300n = 600n = 900\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 300)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 600)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 900)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if we keep adding more, they should continue to improve, right?\nWrong.\nThis is where the Shadow CLT makes its appearance.\nAs our sample size gets bigger—so big that it begins to approach the size of the full population—things start to go awry. What do you think happens when we include all data from the population EXCEPT for just 1 or 2 data points? Make your best guess, then check the tabs to find out! It’s a little trickier than you might expect ;)\n\nn = 970n = 998n = 999\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 970)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 998)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 999)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\n\n\n\nAs our sample size approaches the full population, the distribution of our sample means actually starts to depart from the normal distribution and converge instead to a perfectly mirrored replication of the full population’s distribution!\nWhy?\nWell, one way to think about this is to consider not the data in the sample, but rather the data left out of the sample. As the size of our sampled data grows closer to the full population, the size of our unsampled data grows closer to 0. In the same way that we need a large enough number of sampled values - each contributing some amount of noise that begins to randomly cancel itself out - for the CLT’s effects to start working, we also need a large enough number of unsampled values to ensure we’re avoiding biased noise in the sampled data.\nIn order for the CLT to hold true, it must apply not only to the data we sample but also to the data we leave out of our sample.\nNow, are you ever going to be in a situation where you need normally distributed means when you already have all but a few data points from the full population? Almost certainly not. But I think this little thought experiment highlights an important point: to truly understand your inferences, you must first understand not only the data you include in your analyses, but also the data you leave out.\nSo that’s the Shadow CLT and I hope this little statistics insight was interesting to think about!"
  },
  {
    "objectID": "posts/clts_all_the_way_down/testing.html",
    "href": "posts/clts_all_the_way_down/testing.html",
    "title": "testing",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "posts/clts_all_the_way_down/testing.html#r-markdown",
    "href": "posts/clts_all_the_way_down/testing.html#r-markdown",
    "title": "testing",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "posts/clts_all_the_way_down/testing.html#including-plots",
    "href": "posts/clts_all_the_way_down/testing.html#including-plots",
    "title": "testing",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "posts/clts_all_the_way_down/index.html#footnotes",
    "href": "posts/clts_all_the_way_down/index.html#footnotes",
    "title": "The shadow CLT isn’t real. It can’t hurt you. Unless..?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMostly, they won’t teach you this in stats class because it’s useless hahaha. When are we ever going to be in a situation where we can collect all but 1 value from the full population? When are we ever going to care about getting an accurate significance estimate when we’re literally just 1 value away from knowing the exact population parameter?↩︎"
  },
  {
    "objectID": "posts/personality_calcs/index.html",
    "href": "posts/personality_calcs/index.html",
    "title": "Calculating Better Buzzfeed Quizzes",
    "section": "",
    "text": "Have you ever wondered which Disney Princess you are? Or which celebrity would secretly hate you? Or even the Taylor Swift song that best fits your personality?\nWell over at Buzzfeed, you have the ability to find out! Each of these quizzes take the answers you enter, make some kind of inference about who you are, and then give you a result based on that inference. Their goal is to assess your personality, then match it to one of the quizzes’ predetermined options. Sounds pretty cool, right?\nUnfortunately, under the hood, it’s not so glamorous. For every quiz question, the available answers are connected to one of the potential outcomes. Every answer you choose contributes one point to the outcome its associated with. At the end of the quiz, whichever outcome has the most points is selected as your best match.\nThat’s so lame.\nI mean, come on. Basic counting? Personality has the potential for so so so much more!\nNow, to be clear, this approach accomplishes Buzzfeed’s goal of easy-to-make-and-take personality quizzes. From a business perspective, it’s clearly a good model. But the focus of this post is not business models. It’s personality models, and in that realm, multivariate statistics allow us to do much better.\nSo, if our goal here is to actually assess someone’s personality and then find the closest match to it, how do we go about doing that?\nThere are two implications in that question and we’ll need to address both before we arrive at our eventual answer. The assumptions are (1) that personality is something we can quantify and, (2) that, once quantified, we have a way to measure how “close” or “far” one personality is from another.\nLet’s address these, one at a time."
  },
  {
    "objectID": "extensions_restart.html",
    "href": "extensions_restart.html",
    "title": "Exploring Data Science",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport plotnine as pn\n\n\nh_males = pd.DataFrame({'height': np.random.normal(loc = 176, scale = 2.7, size = 10000)})\nh_females = pd.DataFrame({'height': np.random.normal(loc = 166, scale = 2.5, size = 10000)})\n\nheights = pd.concat([h_males, h_females], axis = 0)\n\n\n(pn.ggplot(heights, pn.aes(x = 'height')) +\n  pn.geom_density(fill = 'grey', alpha = .7) +\n  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +\n  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights') +\n  pn.theme_bw() + \n  pn.theme(axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_text_x = pn.element_text(size = 12),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),\n        title = pn.element_text(size = 16))\n)\n\n\n\n\n\n\n\n\n\n# Create dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n\n    sample = heights.sample(n = 30)\n\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n&lt;positron-console-cell-18&gt;:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n\n\n(\npn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\nnp.random.seed(11)\n\nh_males = pd.DataFrame({'x1': np.random.normal(loc = 177, scale = 2.7, size = 500)})\nh_females = pd.DataFrame({'x1': np.random.normal(loc = 164, scale = 2, size = 500)})\n\nsims = pd.concat([h_males, h_females], axis = 0)\n\n\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 999, replace = False)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n&lt;positron-console-cell-23&gt;:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation."
  },
  {
    "objectID": "posts/personality_calcs/index.html#footnotes",
    "href": "posts/personality_calcs/index.html#footnotes",
    "title": "Calculating Better Buzzfeed Quizzes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, Mahalanobis distances use covariance matrices instead of correlation matrices, but the underlying concept is the same.↩︎"
  }
]