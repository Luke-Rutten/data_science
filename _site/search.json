[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Luke Rutten",
    "section": "",
    "text": "I’m a 4th year PhD student with scientific expertise in applied statistics as well as human learning, behavior, and motivation. I have a passion for using data to uncover the realities hidden right in front of our eyes and striving - always - to learn and grow as a professional.\nCurrently seeking Summer 2025 Internships.\nEmail me at lrutten20@gmail.com or message me on LinkedIn!\n\nExperience\n\nIntuit | Data Science Intern\n\nConducted large scale simulation studies to validate the use of Bayesian AB testing\nLeveraged PySpark with parallelized cloud computing to cut simulation run time by 94%\nDelivered clear recommendations for default Bayesian priors that would increase decision accuracy by 5-15% and precision of effect size estimates by 2-5x across anticipated business conditions\n\n\n\nUT - Austin | Graduate Research Assistant\n\nDesigned and conducted 8 original studies and assisted with data analysis on 5 additional studies\nSynthesized key takeaways to present at 2 national level conferences, with 4 papers accepted for publication at high impact journals\n\n\n\nUT - Austin | Instructor of Record\n\nDesigned and delivered 30 lectures for a semester-long course, synthesizing complex statistical and theoretical concepts on the working of the human mind\nEarned an average instructor rating, across 84 undergraduate students, of 4.9 / 5\n\n\n\n\nEducation\n\nPhD in Educational Psychology | The University of Texas at Austin | 2021 - 2026\nMA in Quantitative Methods | The University of Texas at Austin | 2021 - 2024\nBS in Psychology & Neurobiology | The University of Wisconsin - Madison | 2017 - 2021"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "",
    "text": "Welcome to the first post of my blog on Data Science!\nYes, yes, I know the title says “practice,” not “data” or “science,” but trust me – it connects. At the time of writing, I’m currently in the process of learning how to be a good (read: hirable) data scientist and it’s likely that I’m going to need to learn a thing or two to get there. As I was having a conversation with someone who knows a lot about how best to go about doing that, I was given two unexpected pieces of advice:\n1) Read “Peak: Secrets From the New Science of Expertise”\n2) Start a blog\nSo. Here I am. Blogging about Peak.\nPeak is a book about developing expertise but, much more accurately than that, it’s a book about practice. Specifically, it’s a book about what the authors call “deliberate practice,” how to do it, and why it’s the most effective path to becoming an expert. This blog post is both a summary of my key takeaways from that book and an extension to how I see it applying to my journey into data science. So, without any further ado, here are some of the highlights from Peak.\nPractice does not make perfect.\nPractice makes permanent.\nPractice takes processes that require conscious attention and creates unconscious copies so that next time, we don’t need to think. We can just do. Practice makes our actions faster. And easier. And automatic. Therefore, to practice effectively, we need to (1) identify the exact behaviors we want automated and (2) practice them until they are.\nNow, the interesting thing about practice is that not all forms are created equal. At its core, practice depends on neurobiological mechanisms designed to help our brains grow faster and more energy efficient. In much the same way that we can tailor our workout plans to build better muscles, we can also tailor our practice to make use of these mechanisms and, in doing so, push ourselves to improve even faster and even further than we would ever have otherwise been able to go.\nThis is the process known as “deliberate practice” and this is the concept that this blog post is dedicated to."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Exploring Data Science",
    "section": "",
    "text": "Calculating Better Buzzfeed Quizzes\n\n\n\n\n\n\nStatistics\n\n\n10 Minute Read\n\n\n\nThe secret ingredient is multivariate statistics\n\n\n\n\n\nNov 5, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\n\n\n\n\n\n\nSimplifying Nested For-Loops in Python and R\n\n\n\n\n\n\nPython\n\n\nR\n\n\nProgramming\n\n\n3 Minute Read\n\n\n\nMake iteration easy to write, easy to read, and easy to parallelize with these simple functions\n\n\n\n\n\nOct 2, 2024\n\n\nLuke Rutten\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html#find-a-good-teacher",
    "href": "posts/welcome/index.html#find-a-good-teacher",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "1) Find a good teacher",
    "text": "1) Find a good teacher\nA good teacher is invaluable. They will be able to help you identify both the most useful things to work on as well as the ways in which you can improve what you’re doing. Oftentimes, a good teacher is someone who knows what you’re trying to accomplish well; someone who’s accomplished it themselves or someone who’s trained to help others reach that accomplishment before. Whatever it is you’re trying to do, it will likely help to have someone who knows more than you directing your efforts as you go."
  },
  {
    "objectID": "posts/welcome/index.html#identify-something-specific-to-work-on",
    "href": "posts/welcome/index.html#identify-something-specific-to-work-on",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "2) Identify something specific to work on",
    "text": "2) Identify something specific to work on\nIdeally, work with your teacher to identify this thing.\nIn order to build a mental representation, we need to make the neurons involved activate together over and over. The best way to make that happen is to do something that depends on the representation… over and over. The more times we get our brain to reach for the representation, the stronger the signal that this group of neurons are connected will be and, therefore, the more quickly those connections will be solidified.\nIf we go into practice without a specific goal in mind, it’s likely that we’ll spend our time doing numerous different things, bouncing around from one task to another. However, approaching practice like this doesn’t give our brain the chance to reach for a specific mental representation over and over and, in line with that, doesn’t give our brain the signal it needs to really solidify the representation.\nFurthermore, without a specific goal, it can be difficult to identify ways in which to adjust what we’re doing to improve towards that goal. Without these adjustments, continued practice will just make it easier and faster to do the things we’re already doing. If what we’re already doing is correct, this is wonderful and no adjustments are needed. If, however, it is instead the case that we’re doing something wrong, then making those neuronal connections stronger actually turns out to be detrimental to us down the line. In all likelihood, if you’re still learning how to do something, your current approach is not optimal. It probably needs to be changed. Identifying specific goals to work towards and making attempts over and over will not only begin to solidify mental representations related to this goal but will also allow you to identify and highlight (and practice) ways to change what you’re doing before it’s been made too permanent."
  },
  {
    "objectID": "posts/welcome/index.html#focus-100-while-you-practice",
    "href": "posts/welcome/index.html#focus-100-while-you-practice",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "3) Focus 100% while you practice",
    "text": "3) Focus 100% while you practice\nThis one is straightforward. The more you can focus on what you’re practicing, the stronger the signal will be that this is important. The stronger that signal is, the quicker the mental representations will be built.\nFurther, letting yourself lapse and get sloppy with attempts will mean the wrong sets of neurons are activating together. In essence, if you’re giving anything less than your best, you’re building mental representations of subpar behaviors. Unfortunately, that’s going to make it that much harder in the future when you’re trying to learn more advanced skills that depend on these lower-level representations. Don’t let yourself be sloppy. Focus, 100%.\nNow, with that said, it’s difficult to push yourself at 100% focus for long periods of time. Instead, limiting practice sessions to about an hour and having multiple sessions separated by generous breaks makes it much more feasible."
  },
  {
    "objectID": "posts/welcome/index.html#get-feedback-and-make-it-as-objective-as-possible",
    "href": "posts/welcome/index.html#get-feedback-and-make-it-as-objective-as-possible",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "4) Get feedback and make it as objective as possible",
    "text": "4) Get feedback and make it as objective as possible\nQuantify your performance. Then use that information to adjust as you go. Are you making progress? At the rate you expect? Is your practice working or have you reached a plateau? Knowing what’s working and what isn’t is essential for creating and updating plans as you go. Unfortunately, humans are notoriously bad at making objective assessments of things. Instead of depending on biased intuitions, get concrete performance feedback whenever possible and use that information to help make decisions."
  },
  {
    "objectID": "posts/welcome/index.html#additional-considerations",
    "href": "posts/welcome/index.html#additional-considerations",
    "title": "Deliberate Practice - Getting Better, Faster",
    "section": "Additional Considerations",
    "text": "Additional Considerations\nAt this point, you’ve received a surface-level summary of deliberate practice, why it works, and how to do it. Though I’ve skipped over many of the details in creating this summary, what follows is a list of what I see as the details that are essential to mention:\n\nSleep is so important to this process. Get a full night’s rest every night, or your brain won’t have the time it needs to use the mechanisms that learning depends on.\nThe social environment exerts a powerful influence on our behaviors. Surround yourself with people who actively encourage and support your growth (not distract you from it). Encourage them, in turn. Creating communities of support helps both you in your own endeavors and the world around you.\nAim to work on skills that are just outside of your comfort zone. They should push you, but—with focus—you should be able to accomplish them. These are the skills for which your mental representations aren’t quite built yet. As you work on them and master them, they will lay the foundations for all the steps that come after and make learning those skills much more efficient.\nIf plateauing, try differently, not harder. Ideally, you’ll already be at 100% effort anyway. When approaching a barrier you can’t seem to overcome, changing your angle may help you see things you weren’t able to before.\nDuring practice, actively searching for mistakes is critical.\n\nThis helps keeps you 100% focused instead of zoning out.\nThis also helps you identify and correct errors before they become too ingrained."
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Luke Rutten",
    "section": "Experience",
    "text": "Experience\n\nIntuit - | - Data Science Intern - | - Summer 2024\n\nConducted large scale simulation studies to determine safe default priors for Bayesian AB testing\nLeveraged massive parallel processing using PySpark in Databricks to reduce simulation run time by 94%\nDelivered clear recommendations for default Bayesian priors to increase decision accuracy by 5-15% and increase effect size estimate precision by 2-5x across anticipated business conditions\n\nUT - Austin - | - Graduate Research Assistant - | - August 2021 - Present\n\nDesigned and conducted 8 original studies and assisted with data analysis on 5 additional studies\n\nMethods included: Hierarchical modeling, latent variable modeling, random forest regression, MANOVA\n\nSynthesized key takeaways to present at 2 national level conferences, with 5 papers under review at high impact journals\n\nUT - Austin | Instructor of Record | 2023 - 2024\n\nDesigned and delivered two 75 minute lectures each week over the course of a 15 week semester\nEarned an average instructor rating of 4.9 / 5"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Luke Rutten",
    "section": "Education",
    "text": "Education\n\nPhD in Educational Psychology | The University of Texas at Austin | Aug 2021 - May 2026\nMA in Quantitative Methods | The University of Texas at Austin | Aug 2021 - May 2026\nBS in Psychology & Neurobiology | The University of Wisconsin - Madison | Aug 2017 - May 2021"
  },
  {
    "objectID": "posts/simplying_for_loops/index.html",
    "href": "posts/simplying_for_loops/index.html",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "",
    "text": "The Quick Answer:\nUse specific functions to create a data frame of the Cartesian product of vectors passed to them, which can then be iterated across with just one loop (or parallelized).\nThe function in Python: itertools.product()\nThe function in R: tidyr::expand_grid()\n\n\nThe Full Explanation:\nLet’s say you’re running a simulation study and you’ve been tasked with gathering performance data across a wide range of conditions. Maybe you’re working for a car company that’s trying to perfect its new line of fuel injectors and you need to assess the efficacy of the product across the full range of:\n\nEngine sizes\nEngine temperatures\nManufacturers\nLevels of remaining fuel\nCar mileages\nCar speeds\n\nTo capture performance across the full range of possible conditions here, you’re going to need to run hundreds or maybe even thousands of simulations! That’s way too many to try running one at a time but, lucky for you, you know a thing or two about programming. So you go ahead and define your parameters, then write some for-loops to iterate across conditions.\nLet’s check out a simplified version of what that might look like.\n\nSet parameters\nYou don’t technically need to define your parameters outside the loop, but doing so makes for code that’s both more readable and easier to modify later.\n\nPythonR\n\n\n\nengine_sizes = [2.4, 3.2]\nengine_temps = [180, 200, 220]\nmanufacturers = ['Toyota', 'Mazda', 'Honda']\nfuel_levels = [0.25, 0.75]\nmiles = [50000, 100000, 150000]\nspeeds = [30, 50, 80]\n\n\n\n\nengine_sizes = c(2.4, 3.2)\nengine_temps = c(180, 200, 220)\nmanufacturers = c(\"Toyota\", \"Mazda\", \"Honda\")\nfuel_levels = c(0.25, 0.75)\nmiles = c(50000, 100000, 150000)\nspeeds = c(30, 50, 80)\n\n\n\n\nAnd now that your parameter space is defined, you can go ahead and iterate across it.\n\n\nNested for-loops\n\nPythonR\n\n\n\n# Write nested for-loops to capture every parameter combination\nfor size in engine_sizes:\n  for temp in engine_temps:\n    for manufac in manufacturers:\n      for fuel in fuel_levels:\n        for mile in miles:\n          for mph in speeds:\n        \n            some_function(arg_1 = size, arg_2 = temp, arg_3 = manufac, arg_4 = fuel, arg_5 = mile, arg_6 = mph)\n\n\n\n\n# Write nested for-loops to capture every parameter combination\nfor (size in engine_sizes){\n  for (temp in temps){\n    for (manufac in manufacturers){\n      for (fuel in fuel_levels){\n        for (mile in miles){\n          for (mph in speeds){\n        \n            some_function(arg_1 = size, arg_2 = temp, arg_3 = manufac, arg_4 = fuel, arg_5 = mile, arg_6 = mph)\n          }\n        }\n      }\n    }\n  }\n}\n\n\n\n\nThis code has been simplified for the sake of the example, but we’re already beginning to see an issue. Code that’s been indented 6 times is just hard to read! Now imagine a problem where you have to take into account not just 6 parameters, but 10 or 20 or even 30.\nIt would be a nightmare.\nBut it’s not like we can avoid iteration just because it’s hard to read, so what can be done?\n\n\nThe Solution: Cartesian Products\nBecause a single for-loop is limited to iteration across just one dimension, we need a way to condense all of our parameters (6, in this example) into just one dimension. This could be done as a data frame, a list of lists, or perhaps even as a dictionary of parameters.\nWhichever route we choose to go, it’s not enough just to include our parameters separate from one another. Rather, we need each value to contain a unique combination of parameters and we need every unique combination to be present.\nLuckily for us, most coding languages (and the communities supporting them) have already laid the groundwork to enable us to do this with ease. In Python, we can use itertools.product() and, in R, we can use tidyr::expand_grid(). These functions take the data frame approach, calculating every unique combination of parameters and storing the output in individual rows of a data frame.\n\nPythonR\n\n\n\nengine_sizes = [2.4, 3.2]\nengine_temps = [180, 200, 220]\n\npd.DataFrame(itertools.product(engine_sizes, engine_temps))\n\n     0    1\n0  2.4  180\n1  2.4  200\n2  2.4  220\n3  3.2  180\n4  3.2  200\n5  3.2  220\n\n\n\n\n\nengine_sizes = c(2.4, 3.2)\nengine_temps = c(180, 200, 220)\n\ntidyr::expand_grid(engine_sizes, engine_temps)\n\n# A tibble: 6 × 2\n  engine_sizes engine_temps\n         &lt;dbl&gt;        &lt;dbl&gt;\n1          2.4          180\n2          2.4          200\n3          2.4          220\n4          3.2          180\n5          3.2          200\n6          3.2          220\n\n\n\n\n\nAnd once we have that, all we need to do is iterate across each row of the newly created data frame! Each row contains all the parameter information we’ll need, so we just extract that information within the loop as we iterate through it.\nNow, putting it all together, here’s what code for that might look like:\n\nPythonR\n\n\n\n# load necessary packages\nimport pandas as pd\nfrom itertools import product\n\n# Create a single data frame using the Cartesian product of your parameters\nparams = pd.DataFrame(product(engine_sizes, engine_temps, manufacturers,\n                              fuel_levels, miles, speeds))\n\n# Run just one loop across each row of your data frame\nfor row in np.arange(params.shape[0]):\n  some_function(arg_1 = params.iloc[row, 0],\n                arg_2 = params.iloc[row, 1],\n                arg_3 = params.iloc[row, 2],\n                arg_4 = params.iloc[row, 3],\n                arg_5 = params.iloc[row, 4],\n                arg_6 = params.iloc[row, 5])\n\n\n\n\n# load necessary packages\nlibrary(tidyverse)\n\n# Create a single data frame using the Cartesian product of your parameters\nparams = expand_grid(engine_sizes, engine_temps, manufacturers,\n                     fuel_levels, miles, speeds)\n\n# Run just one loop across each row of your data frame\nfor (row in nrow(params)) {\n  some_function(arg_1 = params[[row, 1]],\n                arg_2 = params[[row, 2]],\n                arg_3 = params[[row, 3]],\n                arg_4 = params[[row, 4]],\n                arg_5 = params[[row, 5]]\n                arg_6 = params[[row, 6]])\n}\n\n\n\n\nAnd it’s really that simple.\nUsing this approach, it doesn’t matter how many parameters you’re working with; the amount of loops you need will only ever be one.\n\n\nThe True Power of this Approach\nAs the experienced reader may have already realized, this approach not only makes it easier to read and write nested for-loops, it also makes it much simpler to parallelize them. With nested loops, you’re limited to parallelizing just one level at a time or worrying about nested threads if you’re brave enough to dare trying multiple. With the Cartesian approach, however, you can reach every combination of parameters with just one parallelized operation, making full use of your computing resources and freeing up your time and attention to think about more important things like how in the world you’re going to visualize all this awesome data you’ve just collected.\nThat, however, is a topic for another post ;)"
  },
  {
    "objectID": "posts/simplying_for_loops/index.html#results",
    "href": "posts/simplying_for_loops/index.html#results",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "Results",
    "text": "Results\n\nPlots\ntesxt\n\n\nTables\ncxd"
  },
  {
    "objectID": "posts/simplying_for_loops/index.html#something-different",
    "href": "posts/simplying_for_loops/index.html#something-different",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "Something different",
    "text": "Something different\nNot a tabset or something\nOne thing to note: with this much power, it’s very easy to allow things to get out of hand. Each new condition increases exponentially the amount of conditions to iterate through.\nBecause this approach allows each condition to run separately from the others, it lends itself directly to parallelization - but that’s a topic for another blog post."
  },
  {
    "objectID": "posts/simplying_for_loops/index.html#parallelization---the-true-power-of-this-approach",
    "href": "posts/simplying_for_loops/index.html#parallelization---the-true-power-of-this-approach",
    "title": "Simplifying Nested For-Loops in Python and R",
    "section": "Parallelization - The True power of this approach",
    "text": "Parallelization - The True power of this approach\nNot a tabset or something\nOne thing to note: with this much power, it’s very easy to allow things to get out of hand. Each new condition increases exponentially the amount of conditions to iterate through.\nBecause this approach allows each condition to run separately from the others, it lends itself directly to parallelization - but that’s a topic for another blog post."
  },
  {
    "objectID": "posts/clts_all_the_way_down/testing.html",
    "href": "posts/clts_all_the_way_down/testing.html",
    "title": "testing",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "posts/clts_all_the_way_down/testing.html#r-markdown",
    "href": "posts/clts_all_the_way_down/testing.html#r-markdown",
    "title": "testing",
    "section": "",
    "text": "This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.\nWhen you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:\n\nsummary(cars)\n\n     speed           dist       \n Min.   : 4.0   Min.   :  2.00  \n 1st Qu.:12.0   1st Qu.: 26.00  \n Median :15.0   Median : 36.00  \n Mean   :15.4   Mean   : 42.98  \n 3rd Qu.:19.0   3rd Qu.: 56.00  \n Max.   :25.0   Max.   :120.00"
  },
  {
    "objectID": "posts/clts_all_the_way_down/testing.html#including-plots",
    "href": "posts/clts_all_the_way_down/testing.html#including-plots",
    "title": "testing",
    "section": "Including Plots",
    "text": "Including Plots\nYou can also embed plots, for example:\n\n\n\n\n\n\n\n\n\nNote that the echo = FALSE parameter was added to the code chunk to prevent printing of the R code that generated the plot."
  },
  {
    "objectID": "posts/personality_calcs/index.html",
    "href": "posts/personality_calcs/index.html",
    "title": "Calculating Better Buzzfeed Quizzes",
    "section": "",
    "text": "Have you ever wondered which Disney Princess you are? Or which celebrity would secretly hate you? Or even the Taylor Swift song that best fits your personality?\nWell over at Buzzfeed, you have the ability to find out! Each of these quizzes take the answers you enter, make some kind of inference about who you are, and then give you a result based on that inference. Their goal is to assess your personality, then match it to one of the quizzes’ predetermined options. Sounds pretty cool, right?\nUnfortunately, under the hood, it’s not so glamorous. For every quiz question, the available answers are connected to one of the potential outcomes. Every answer you choose contributes one point to the outcome its associated with. At the end of the quiz, whichever outcome has the most points is selected as your best match.\nThat’s so lame.\nI mean, come on. Basic counting? Personality has the potential for so so so much more!\nNow, to be clear, this approach accomplishes Buzzfeed’s goal of easy-to-make-and-take personality quizzes. From a business perspective, it’s clearly a good model. But the focus of this post is not business models. It’s personality models, and in that realm, multivariate statistics allow us to do much better.\nSo, if our goal here is to actually assess someone’s personality and then find the closest match to it, how do we go about doing that?\nThere are two implications in that question and we’ll need to address both before we arrive at our eventual answer. The assumptions are (1) that personality is something we can quantify and, (2) that, once quantified, we have a way to measure how “close” or “far” one personality is from another.\nLet’s address these, one at a time."
  },
  {
    "objectID": "extensions_restart.html",
    "href": "extensions_restart.html",
    "title": "Exploring Data Science",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport plotnine as pn\n\n\nh_males = pd.DataFrame({'height': np.random.normal(loc = 176, scale = 2.7, size = 10000)})\nh_females = pd.DataFrame({'height': np.random.normal(loc = 166, scale = 2.5, size = 10000)})\n\nheights = pd.concat([h_males, h_females], axis = 0)\n\n\n(pn.ggplot(heights, pn.aes(x = 'height')) +\n  pn.geom_density(fill = 'grey', alpha = .7) +\n  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +\n  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights') +\n  pn.theme_bw() + \n  pn.theme(axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_text_x = pn.element_text(size = 12),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),\n        title = pn.element_text(size = 16))\n)\n\n\n\n\n\n\n\n\n\n# Create dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n\n    sample = heights.sample(n = 30)\n\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n&lt;positron-console-cell-18&gt;:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n\n\n\n(\npn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n\n\n\n\n\n\n\n\nnp.random.seed(11)\n\nh_males = pd.DataFrame({'x1': np.random.normal(loc = 177, scale = 2.7, size = 500)})\nh_females = pd.DataFrame({'x1': np.random.normal(loc = 164, scale = 2, size = 500)})\n\nsims = pd.concat([h_males, h_females], axis = 0)\n\n\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 999, replace = False)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n\n&lt;positron-console-cell-23&gt;:9: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation."
  },
  {
    "objectID": "posts/personality_calcs/index.html#footnotes",
    "href": "posts/personality_calcs/index.html#footnotes",
    "title": "Calculating Better Buzzfeed Quizzes",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTechnically, Mahalanobis distances use covariance matrices instead of correlation matrices, but the underlying concept is the same.↩︎"
  }
]