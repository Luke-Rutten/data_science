---
title: "The shadow CLT"
draft: true
description: "A statistical secret hiding within every population"
author: "Luke Rutten"
format:
  html:
    code-fold: true
    code-overflow: wrap
execute:
  warning: false
date: "2024-10-22"
image: "stand_in.png"
categories:
  - Statistics
  - 3 Minute Read
---

I should start by saying that although the Shadow CLT is a real phenomenon, you'll probably never find yourself in a situation where it matters. However, I also think it gives some insights into how sampling, populations, and the CLT work - so, if that sounds of interest to you, I encourage you to read on!

# The Central Limit Theorem

> As you collect samples from a population, the Central Limit Theorem states that the distribution of the means of those samples will converge to a normal distribution—regardless of the underlying population distribution—so long as the sample sizes are large enough.

Now that CLT definition is a bit abstract so, to make it more clear, here's a demonstration. Let's say we're trying to make an inference about the average height of students in their freshman year, but there's a small problem: human height tends to follow a bimodal distribution (one peak for males, one peak for females).

```{python}
# Load libraries
import numpy as np
import pandas as pd
import plotnine as pn

# Simulate data
h_males = pd.DataFrame({'height': np.random.normal(loc = 176, scale = 2.7, size = 10000)})
h_females = pd.DataFrame({'height': np.random.normal(loc = 166, scale = 2.5, size = 10000)})

heights = pd.concat([h_males, h_females], axis = 0)

# Plot data
(pn.ggplot(heights, pn.aes(x = 'height')) +
  pn.geom_density(fill = 'grey', alpha = .7) +
  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +
  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights') +
  pn.theme_bw() + 
  pn.theme(axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_text_x = pn.element_text(size = 12),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),
        title = pn.element_text(size = 16))
)
```

The non-normal nature of the data is going to affect our variance estimates which will, in turn, affect our significance estimates. Not to worry though! The Central Limit Theorem states that, even from this non-normal distribution, if we were to sample from it over and over and then plot the means of those samples, the outcome would end up normal so long as the size of our samples is large enough.

Let's try it! We'll take samples of varying sizes (*n*), calculate the sample averages, and then plot them out and see what happens:

::: panel-tabset
### *n* = 1

```{python}
# Create dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = heights.sample(n = 1)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 30

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = heights.sample(n = 30)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 300

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = heights.sample(n = 300)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```
:::

It works!

Even when drawing samples from a population that is very much not normally distributed, the means of our samples ended up looking normal. The CLT says this will hold true pretty much no matter what the population distribution looks like; if it has a mean and a variance, the sample means will end up normal as long as your sample size is large enough. That's a good thing too, because many of our statistical practices actually depend on the assumption that this holds up.

And, in most cases, it does. Except for, well...

# The Shadow CLT

It turns out there's a unique spot within every population where the CLT begins to fall apart and why this happens is because of a secret, inverted CLT which I'm unofficially terming "The Shadow CLT" (mostly because I find that name funny hahaha). Rather than dive into a conceptual explanation, though, let me just show the Shadow CLT in action:

We start with a full population of 1,000 data points. It doesn't need to be 1,000; it could be 42 or 1,000,000 or really however many you feel like. I'm going with 1,000 because it's easy to graph. Now, in this example, our thousand data points follow a non-normal distribution like before. Maybe we're trying to make an inference about the average height of freshman attending a specific school this time and our population data looks something like this:

```{python}
# Simulate data reproducibly
np.random.seed(11)

h_males = pd.DataFrame({'x1': np.random.normal(loc = 177, scale = 2.7, size = 500)})
h_females = pd.DataFrame({'x1': np.random.normal(loc = 164, scale = 2, size = 500)})

sims = pd.concat([h_males, h_females], axis = 0)

# Plot it
(pn.ggplot(sims, pn.aes(x = 'x1')) +
  pn.geom_density(fill = 'grey', alpha = .7) +
  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +
  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights (1,000 Students)') +
  pn.theme_bw() + 
  pn.theme(axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_text_x = pn.element_text(size = 12),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),
        title = pn.element_text(size = 16))
)
```

Remember, the central limit theorem says that as long as our sample size is large enough, the distribution of our sample means *should* end up looking normal here.

The Shadow CLT doesn't quite agree, but before we get to that let's first establish that the normal CLT works within this fictional dataset of 1,000 freshmen. Again, we'll take a bunch of samples of various sizes (*n*) and plot them out so we can see how they're distributed.

::: panel-tabset
### *n* = 1

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 1)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 2

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 2)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 30

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 30)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```
:::

Great! Everything is looking good so far. Our *n* = 1 and *n* = 2 distributions weren't exactly normal but that's because the sample sizes weren't large enough for the CLT to take effect. Once we got up to samples of 30, our distribution started to take on a nice, normal shape.

And as we add even more data to our samples, things only get better.

::: panel-tabset
### *n* = 300

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 300)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 600

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 600)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 900

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 900)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```
:::

And if we keep adding more, they should continue to improve, right?

Wrong.

This is where the Shadow CLT makes its appearance.

As our sample size gets bigger—so big that it begins to approach the size of the full population—things start to go awry. What do you think happens when we include all data from the population *EXCEPT* for just 1 or 2 data points? Make your best guess, then check the tabs to find out! It's a little trickier than you might expect ;)

::: panel-tabset
### *n* = 970

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 970)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 998

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 998)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```

### *n* = 999

```{python}
# Reset dataframe to store outputs in
outcomes = pd.DataFrame(columns = ["mean_c"])

# Iterate sampling procedure (take 10,000 samples)
for i in range(10000):
    np.random.seed(i)
    sample = sims.sample(n = 999)
    sample_mean = pd.DataFrame({"mean_c": [np.mean(sample)]})
    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)

# Plot the sample means
(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +
  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +
  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +
  pn.theme_bw() +
  pn.theme(axis_text_x = pn.element_text(size = 10),
        axis_text_y = pn.element_blank(),
        axis_ticks_y = pn.element_blank(),
        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),
        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))
)
```
:::

As our sample size approaches the full population, the distribution of our sample means actually starts to depart from the normal distribution and converge instead to a *perfectly mirrored* replication of the full population's distribution!

Why?

Well, one way to think about this is to consider not the data in the sample, but rather the data *left out of the sample*. As the size of our sampled data grows closer to the full population, the size of our *unsampled* data grows closer to 0. In the same way that we need a large enough number of sampled values - each contributing some amount of noise that begins to randomly cancel itself out - for the CLT's effects to start working, we also need a large enough number of *unsampled* values to ensure we're avoiding biased noise in the sampled data.

In order for the CLT to hold true, it must apply not only to the data we sample but *also* to the data we leave out of our sample.

Now, are you ever going to be in a situation where you need normally distributed means when you already have all but a few data points from the full population? Almost certainly not. But I think this little thought experiment highlights an important point: to truly understand your inferences, you must first understand not only the data you include in your analyses, but also the data you leave out.

So that's the Shadow CLT and I hope this little statistics insight was interesting to think about!