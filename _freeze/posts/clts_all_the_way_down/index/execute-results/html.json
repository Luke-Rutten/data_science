{
  "hash": "39eb56517241b8a6edb5e677e849a4ff",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"The shadow CLT\"\ndescription: \"A statistical secret hiding within every population\"\nauthor: \"Luke Rutten\"\nformat:\n  html:\n    code-fold: true\n    code-overflow: wrap\nexecute:\n  warning: false\ndate: \"2024-10-22\"\nimage: \"stand_in.png\"\ncategories:\n  - Statistics\n  - 3 Minute Read\n---\n\n\nI should start by saying that although the Shadow CLT is a real phenomenon, you'll probably never find yourself in a situation where it matters. However, I also think it gives some insights into how sampling, populations, and the CLT work - so, if that sounds of interest to you, I encourage you to read on!\n\n# The Central Limit Theorem\n\n> As you collect samples from a population, the Central Limit Theorem states that the distribution of the means of those samples will converge to a normal distribution—regardless of the underlying population distribution—so long as the sample sizes are large enough.\n\nNow that CLT definition is a bit abstract so, to make it more clear, here's a demonstration. Let's say we're trying to make an inference about the average height of students in their freshman year, but there's a small problem: human height tends to follow a bimodal distribution (one peak for males, one peak for females).\n\n::: {#f18062cf .cell execution_count=1}\n``` {.python .cell-code}\n# Load libraries\nimport numpy as np\nimport pandas as pd\nimport plotnine as pn\n\n# Simulate data\nh_males = pd.DataFrame({'height': np.random.normal(loc = 176, scale = 2.7, size = 10000)})\nh_females = pd.DataFrame({'height': np.random.normal(loc = 166, scale = 2.5, size = 10000)})\n\nheights = pd.concat([h_males, h_females], axis = 0)\n\n# Plot data\n(pn.ggplot(heights, pn.aes(x = 'height')) +\n  pn.geom_density(fill = 'grey', alpha = .7) +\n  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +\n  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights') +\n  pn.theme_bw() + \n  pn.theme(axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_text_x = pn.element_text(size = 12),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),\n        title = pn.element_text(size = 16))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=672 height=480}\n:::\n:::\n\n\nThe non-normal nature of the data is going to affect our variance estimates which will, in turn, affect our significance estimates. Not to worry though! The Central Limit Theorem states that, even from this non-normal distribution, if we were to sample from it over and over and then plot the means of those samples, the outcome would end up normal so long as the size of our samples is large enough.\n\nLet's try it! We'll take samples of varying sizes (*n*), calculate the sample averages, and then plot them out and see what happens:\n\n::: panel-tabset\n### *n* = 1\n\n::: {#1119fb0d .cell execution_count=2}\n``` {.python .cell-code}\n# Create dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = heights.sample(n = 1)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 30\n\n::: {#8dd2a9f8 .cell execution_count=3}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = heights.sample(n = 30)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 300\n\n::: {#594dfa55 .cell execution_count=4}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = heights.sample(n = 300)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=672 height=480}\n:::\n:::\n\n\n:::\n\nIt works!\n\nEven when drawing samples from a population that is very much not normally distributed, the means of our samples ended up looking normal. The CLT says this will hold true pretty much no matter what the population distribution looks like; if it has a mean and a variance, the sample means will end up normal as long as your sample size is large enough. That's a good thing too, because many of our statistical practices actually depend on the assumption that this holds up.\n\nAnd, in most cases, it does. Except for, well...\n\n# The Shadow CLT\n\nIt turns out there's a unique spot within every population where the CLT begins to fall apart and why this happens is because of a secret, inverted CLT which I'm unofficially terming \"The Shadow CLT\" (mostly because I find that name funny hahaha). Rather than dive into a conceptual explanation, though, let me just show the Shadow CLT in action:\n\nWe start with a full population of 1,000 data points. It doesn't need to be 1,000; it could be 42 or 1,000,000 or really however many you feel like. I'm going with 1,000 because it's easy to graph. Now, in this example, our thousand data points follow a non-normal distribution like before. Maybe we're trying to make an inference about the average height of freshman attending a specific school this time and our population data looks something like this:\n\n::: {#b1f6def3 .cell execution_count=5}\n``` {.python .cell-code}\n# Simulate data reproducibly\nnp.random.seed(11)\n\nh_males = pd.DataFrame({'x1': np.random.normal(loc = 177, scale = 2.7, size = 500)})\nh_females = pd.DataFrame({'x1': np.random.normal(loc = 164, scale = 2, size = 500)})\n\nsims = pd.concat([h_males, h_females], axis = 0)\n\n# Plot it\n(pn.ggplot(sims, pn.aes(x = 'x1')) +\n  pn.geom_density(fill = 'grey', alpha = .7) +\n  pn.scale_x_continuous(breaks = [160, 165, 170, 175, 180, 185]) +\n  pn.labs(x = 'Height (cm)', y = 'Probability', title = 'Population Distribution of Heights (1,000 Students)') +\n  pn.theme_bw() + \n  pn.theme(axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_text_x = pn.element_text(size = 12),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 18}),\n        title = pn.element_text(size = 16))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=672 height=480}\n:::\n:::\n\n\nRemember, the central limit theorem says that as long as our sample size is large enough, the distribution of our sample means *should* end up looking normal here.\n\nThe Shadow CLT doesn't quite agree, but before we get to that let's first establish that the normal CLT works within this fictional dataset of 1,000 freshmen. Again, we'll take a bunch of samples of various sizes (*n*) and plot them out so we can see how they're distributed.\n\n::: panel-tabset\n### *n* = 1\n\n::: {#42c46323 .cell execution_count=6}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 1)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 2\n\n::: {#0a23839b .cell execution_count=7}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 2)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 30\n\n::: {#dede9b03 .cell execution_count=8}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 30)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=672 height=480}\n:::\n:::\n\n\n:::\n\nGreat! Everything is looking good so far. Our *n* = 1 and *n* = 2 distributions weren't exactly normal but that's because the sample sizes weren't large enough for the CLT to take effect. Once we got up to samples of 30, our distribution started to take on a nice, normal shape.\n\nAnd as we add even more data to our samples, things only get better.\n\n::: panel-tabset\n### *n* = 300\n\n::: {#44eec287 .cell execution_count=9}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 300)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 600\n\n::: {#f08920c8 .cell execution_count=10}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 600)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 900\n\n::: {#fe941062 .cell execution_count=11}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 900)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=672 height=480}\n:::\n:::\n\n\n:::\n\nAnd if we keep adding more, they should continue to improve, right?\n\nWrong.\n\nThis is where the Shadow CLT makes its appearance.\n\nAs our sample size gets bigger—so big that it begins to approach the size of the full population—things start to go awry. What do you think happens when we include all data from the population *EXCEPT* for just 1 or 2 data points? Make your best guess, then check the tabs to find out! It's a little trickier than you might expect ;)\n\n::: panel-tabset\n### *n* = 970\n\n::: {#1e2e2e84 .cell execution_count=12}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 970)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-13-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 998\n\n::: {#b7c7d9c2 .cell execution_count=13}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 998)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-14-output-1.png){width=672 height=480}\n:::\n:::\n\n\n### *n* = 999\n\n::: {#6d1387c8 .cell execution_count=14}\n``` {.python .cell-code}\n# Reset dataframe to store outputs in\noutcomes = pd.DataFrame(columns = [\"mean_c\"])\n\n# Iterate sampling procedure (take 10,000 samples)\nfor i in range(10000):\n    np.random.seed(i)\n    sample = sims.sample(n = 999)\n    sample_mean = pd.DataFrame({\"mean_c\": [np.mean(sample)]})\n    outcomes = pd.concat([outcomes, sample_mean], ignore_index = True)\n\n# Plot the sample means\n(pn.ggplot(outcomes, pn.aes(x = 'mean_c')) +\n  pn.geom_histogram(bins = 30, fill = 'grey', color = 'black') +\n  pn.labs(x = 'Average Height of the Sample (cm)', y = 'Relative Count') +\n  pn.theme_bw() +\n  pn.theme(axis_text_x = pn.element_text(size = 10),\n        axis_text_y = pn.element_blank(),\n        axis_ticks_y = pn.element_blank(),\n        axis_title_y = pn.element_text(size = 14, margin = {'r': 4}),\n        axis_title_x = pn.element_text(size = 14, margin = {'t': 14}))\n)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-15-output-1.png){width=672 height=480}\n:::\n:::\n\n\n:::\n\nAs our sample size approaches the full population, the distribution of our sample means actually starts to depart from the normal distribution and converge instead to a *perfectly mirrored* replication of the full population's distribution!\n\nWhy?\n\nWell, one way to think about this is to consider not the data in the sample, but rather the data *left out of the sample*. As the size of our sampled data grows closer to the full population, the size of our *unsampled* data grows closer to 0. In the same way that we need a large enough number of sampled values - each contributing some amount of noise that begins to randomly cancel itself out - for the CLT's effects to start working, we also need a large enough number of *unsampled* values to ensure we're avoiding biased noise in the sampled data.\n\nIn order for the CLT to hold true, it must apply not only to the data we sample but *also* to the data we leave out of our sample.\n\nNow, are you ever going to be in a situation where you need normally distributed means when you already have all but a few data points from the full population? Almost certainly not. But I think this little thought experiment highlights an important point: to truly understand your inferences, you must first understand not only the data you include in your analyses, but also the data you leave out.\n\nSo that's the Shadow CLT and I hope this little statistics insight was interesting to think about!\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}